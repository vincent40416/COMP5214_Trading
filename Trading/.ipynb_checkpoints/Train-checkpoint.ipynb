{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm, trange\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "from torch.optim.lr_scheduler import MultiplicativeLR\n",
    "import matplotlib.pyplot as plt\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Device Setting & data directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = os.getcwd()\n",
    "data_dir = os.path.dirname(path)\n",
    "device = torch.device('cuda:0')\n",
    "batch_size = 64"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading transformers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at activebus/BERT-DK_rest were not used when initializing BertModel: ['bert.pooler.dense.weight', 'bert.pooler.dense.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertModel(\n",
       "  (embeddings): BertEmbeddings(\n",
       "    (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "    (position_embeddings): Embedding(512, 768)\n",
       "    (token_type_embeddings): Embedding(2, 768)\n",
       "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder): BertEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (1): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (2): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (3): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (4): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (5): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (6): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (7): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (8): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (9): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (10): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (11): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load_transformer\n",
    "print('Loading transformers...')\n",
    "transformer_tag = \"activebus/BERT-DK_rest\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(transformer_tag)\n",
    "transformer = AutoModel.from_pretrained(transformer_tag, add_pooling_layer=False)\n",
    "transformer.to(device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Combined_News_DJIA data...\n",
      "Loading upload_DJIA_table data...\n",
      "Loaded\n"
     ]
    }
   ],
   "source": [
    "def load_data(split_name='train'):\n",
    "    print('Loading {} data...'.format(split_name))\n",
    "    df = pd.read_csv(data_dir + f'/{split_name}.csv')\n",
    "    return df\n",
    "df = load_data('Combined_News_DJIA')\n",
    "stock_df = load_data('upload_DJIA_table')\n",
    "df['price'] = stock_df.Close\n",
    "\n",
    "train_df = df[:int(len(df) * 0.9)]\n",
    "valid_df = df[int(len(df) * 0.9):]\n",
    "num_train_batches = (len(train_df) + batch_size - 1) // batch_size\n",
    "num_valid_batches = (len(valid_df) + batch_size - 1) // batch_size\n",
    "# news = pd.read_csv(\"../Combined_News_DJIA.csv\")\n",
    "\n",
    "print(\"Loaded\")\n",
    "# train_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model \n",
    "Pre_trained_BERT Finished <br>\n",
    "Need Linear & LSTM\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, hidden_size, num_linear=1):\n",
    "        super(Model, self).__init__()\n",
    "        self.pooler = nn.Sequential(\n",
    "            nn.Linear(768, 768),\n",
    "            nn.Tanh(),\n",
    "        )\n",
    "        self.layers = nn.ModuleList([nn.Linear(768 + 1, hidden_size)])\n",
    "        self.layers.extend([nn.Linear(hidden_size, hidden_size) for i in range(num_linear)])\n",
    "        self.dropout = nn.Dropout(p=0.5)\n",
    "        self.output = nn.Linear(hidden_size, 1)\n",
    "        \n",
    "#         Add LSTM HERE\n",
    "        self.LSTM = nn.LSTM(input_size=769,hidden_size=769,\n",
    "                           num_layers=2,batch_first=True)\n",
    "#        End LSTM HERE\n",
    "    def forward(self, text_embeddings, other_inputs):\n",
    "        pooled_output = self.pooler(text_embeddings)  # (batch_size, 768)\n",
    "        h = torch.cat([pooled_output, other_inputs], 1)  # (batch_size, 768 + 1)\n",
    "        \n",
    "#         Add LSTM HERE\n",
    "        h_size = list(h.shape)\n",
    "        h = h.reshape(h_size[0],1,h_size[1])\n",
    "\n",
    "        h, (hn, cn) = self.LSTM(h, None)\n",
    "        h = h[:,-1,:]\n",
    "#         End LSTM HERE\n",
    "\n",
    "        for layer in self.layers:\n",
    "            h = nn.functional.leaky_relu(layer(h))  # (batch_size, hidden_size)\n",
    "            h = self.dropout(h)\n",
    "        o = self.output(h)  # (batch_size, 1)\n",
    "        return o"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train hyperparameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 10\n",
    "lr = 0.01\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "hidden_size = 128\n",
    "num_linear = 1\n",
    "\n",
    "records = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_predictions(outputs):\n",
    "    logits = torch.sigmoid(outputs)\n",
    "    threshold = 0.5\n",
    "    predictions = torch.zeros(len(logits),1).to(device)\n",
    "    for i in range(len(logits)):\n",
    "        if logits[i] < threshold:\n",
    "            predictions[i] = 0\n",
    "        else:\n",
    "            predictions[i] = 1\n",
    "    return predictions\n",
    "\n",
    "# Brute concatenate TopK News\n",
    "def make_input_batch(i_batch, df, batch_size):\n",
    "    rows = df[i_batch* batch_size : min((i_batch+1) * batch_size, len(df))]\n",
    "    text = rows[rows.columns.difference(['price', 'Label', 'Date'])]\n",
    "    text = text.apply(lambda x :' '.join(x.astype(str)),1).tolist()\n",
    "    text_inputs = tokenizer(text, padding='max_length', truncation=True, return_tensors=\"pt\")\n",
    "    text_inputs = {k : v.to(device) for k, v in text_inputs.items()}\n",
    "    with torch.no_grad():\n",
    "        text_embeddings = transformer(**text_inputs, return_dict=True).last_hidden_state[:,0,:]\n",
    "    other_inputs = torch.tensor([rows.price.tolist()], dtype=torch.float32).to(device) # (batch_size, 1)\n",
    "    other_inputs = torch.transpose(other_inputs, 0, 1)\n",
    "    train_labels = torch.tensor([rows.Label.tolist()],dtype=torch.float32).to(device)  \n",
    "    train_labels = torch.transpose(train_labels, 0, 1) # (batch_size, 1)\n",
    "    return text_embeddings, other_inputs, train_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hid-128-numlin-1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "ep  1:   0%|                                                                                    | 0/28 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "Start Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ep  1: 100%|███████████████████████████████████████████████████████████████████████████| 28/28 [01:18<00:00,  2.82s/it]\n",
      "valid:   0%|                                                                                     | 0/4 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " loss: 0.010821,  train acc: 0.534078\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "valid: 100%|█████████████████████████████████████████████████████████████████████████████| 4/4 [00:08<00:00,  2.18s/it]\n",
      "ep  2:   0%|                                                                                    | 0/28 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid acc 0.5376884422110553\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ep  2: 100%|███████████████████████████████████████████████████████████████████████████| 28/28 [01:20<00:00,  2.86s/it]\n",
      "ep  3:   0%|                                                                                    | 0/28 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " loss: 0.010808,  train acc: 0.535196\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ep  3:  43%|████████████████████████████████▏                                          | 12/28 [00:37<00:50,  3.15s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-54bc6f397a1e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mi_batch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnum_train_batches\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdesc\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'ep {:2d}'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 30\u001b[1;33m         \u001b[0mtext_embeddings\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mother_inputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_labels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmake_input_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi_batch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_df\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     31\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m         \u001b[1;31m# train step\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-7-3267f7b12a09>\u001b[0m in \u001b[0;36mmake_input_batch\u001b[1;34m(i_batch, df, batch_size)\u001b[0m\n\u001b[0;32m     19\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m         \u001b[0mtext_embeddings\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtransformer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mtext_inputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreturn_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlast_hidden_state\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 21\u001b[1;33m     \u001b[0mother_inputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mrows\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprice\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# (batch_size, 1)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     22\u001b[0m     \u001b[0mother_inputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mother_inputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m     \u001b[0mtrain_labels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mrows\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mLabel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# save path\n",
    "config = f'hid-{hidden_size}-numlin-{num_linear}'\n",
    "print(config)\n",
    "save_path = data_dir + '/results/' + config\n",
    "\n",
    "# model design\n",
    "model = Model(hidden_size=hidden_size, num_linear=num_linear)\n",
    "# if os.path.isfile(save_path + '/model.pt'):\n",
    "#     model.load_state_dict(torch.load(save_path + '/model.pt'))\n",
    "model.to(device)\n",
    "\n",
    "# Learning Rate\n",
    "optimizer = optim.SGD(model.parameters(), lr=lr, momentum=0.9)\n",
    "lmbda = lambda epoch: 0.95\n",
    "scheduler = MultiplicativeLR(optimizer, lr_lambda=lmbda)\n",
    "total_acc = 0\n",
    "print(total_acc)\n",
    "losses, acc_train, acc_valid = [], [], []\n",
    "print(\"Start Training\")\n",
    "for epoch in range(num_epochs):\n",
    "    # train\n",
    "    running_loss = 0.0\n",
    "    total_acc = 0\n",
    "    model.train()\n",
    "\n",
    "    # shuffle train data\n",
    "    train_df = train_df.sample(frac=1, random_state=epoch).reset_index(drop=True)\n",
    "\n",
    "    for i_batch in trange(num_train_batches, desc='ep {:2d}'.format(epoch + 1)):\n",
    "        text_embeddings, other_inputs, train_labels = make_input_batch(i_batch, train_df, batch_size)\n",
    "\n",
    "        # train step\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(text_embeddings, other_inputs)\n",
    "        \n",
    "        loss = criterion(outputs, train_labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # record\n",
    "        losses.append(loss.cpu().detach().item())\n",
    "        running_loss += losses[-1]\n",
    "        predictions = compute_predictions(outputs)\n",
    "        total_acc += (predictions == train_labels).sum().item()\n",
    "\n",
    "    print(' loss: %.6f,  train acc: %.6f' % (running_loss / len(train_df), total_acc / len(train_df)))\n",
    "    acc_train.append(total_acc / len(train_df))\n",
    "    scheduler.step()\n",
    "\n",
    "    # validate\n",
    "    if epoch % 3 == 0:\n",
    "        model.eval()\n",
    "        total_acc = 0\n",
    "        ## need to find valid data\n",
    "        with torch.no_grad():\n",
    "            for i_batch in trange(num_valid_batches, desc='valid'):\n",
    "                text_embeddings, other_inputs, valid_labels = make_input_batch(i_batch, valid_df, batch_size)\n",
    "                outputs = model(text_embeddings, other_inputs)\n",
    "                predictions = compute_predictions(outputs)\n",
    "                total_acc += (predictions == valid_labels).sum().item()\n",
    "\n",
    "        print('valid acc', total_acc / len(valid_df))\n",
    "        acc_valid.append(total_acc / len(valid_df))\n",
    "\n",
    "print('Finished Training')\n",
    "\n",
    "if not os.path.isdir(save_path):\n",
    "    os.mkdir(save_path)\n",
    "torch.save(model.state_dict(), save_path + '/model.pt')\n",
    "\n",
    "record = json.dumps({\n",
    "    'losses' : losses, 'acc_train' : acc_train, 'acc_valid' : acc_valid, \n",
    "    'num_epochs' : num_epochs, 'batch_size' : batch_size, 'lr' : lr, 'hidden_size':hidden_size, 'num_linear':num_linear,\n",
    "    'transformer_tag' : transformer_tag\n",
    "}, sort_keys=True, indent=4)\n",
    "records.append(record)\n",
    "with open(save_path + f'/record-{num_epochs}.json', 'w') as f:\n",
    "    f.write(record)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Record"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "records_p = data_dir + f'results/records-{num_epochs}.json'\n",
    "\n",
    "with open(records_p, 'r') as f:\n",
    "    records = json.loads(f.read())\n",
    "records = [json.loads(record) for record in records]\n",
    "records = {(r['hidden_size'], r['num_linear']) : r for r in records}\n",
    "\n",
    "fig, axs = plt.subplots(len(hidden_sizes), len(num_linears), sharex=True, sharey=True, figsize=(12,8))\n",
    "axs = axs if len(hidden_sizes) == 1 and len(num_linears) == 1 else axs[ihs, inl]\n",
    "for ihs, hidden_size in enumerate(hidden_sizes):\n",
    "    for inl, num_linear in enumerate(num_linears):\n",
    "        r = records[(hidden_size, num_linear)]\n",
    "        axs.plot([i for i in range(num_epochs)], r['acc_train'])\n",
    "        axs.plot([i for i in range(0, num_epochs, 3)], r['acc_valid'], 'tab:orange')\n",
    "        axs.set_title('hs={}, nl={} ({:.1f})'.format(hidden_size, num_linear, r['acc_valid'][-1]*100))\n",
    "\n",
    "# for ax in axs.flat:\n",
    "#     ax.set(xlabel='epochs', ylabel='accuracy')\n",
    "#     # Hide x labels and tick labels for top plots and y ticks for right plots.\n",
    "#     ax.label_outer()\n",
    "\n",
    "fig.savefig(data_dir + f'results/plt-{num_epochs}.png')\n",
    "fig.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
